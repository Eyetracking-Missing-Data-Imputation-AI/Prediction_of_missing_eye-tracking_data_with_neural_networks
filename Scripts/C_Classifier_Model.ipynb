{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "307c2795",
   "metadata": {},
   "source": [
    "#### Prediction of missing eye tracking data with neural networks\n",
    "Lucas D. Haberkamp<sup>1,2,3</sup>, Michael D. Reddix<sup>1</sup>\n",
    "\n",
    "<sup>1</sup>Naval Medical Research Unit - Dayton  \n",
    "<sup>2</sup>Oak Ridge Institute for Science and Education  \n",
    "<sup>3</sup>Leidos   \n",
    "\n",
    "---\n",
    "\n",
    "This script trains a multi-layer perceptron to classify the gaze label from the 3D point of gaze coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04776dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddd5d15",
   "metadata": {},
   "source": [
    "\"extractdf\" is a function which reads each file as a dataframe and saves the dataframe to a list and also saves a separate list of the filenames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14013f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill_nan should be used with classification ground truth data. prevents a nan label.\n",
    "def extractdf(data_path, fill_nan=False):\n",
    "    file_list, df_list = [], [] # initialize lists\n",
    "    for filename in os.listdir(data_path):\n",
    "        f = os.path.join(data_path, filename) \n",
    "        if os.path.isfile(f):\n",
    "            current_file = os.path.splitext(filename)[0].split('_')[0] # get the identifier of the participant from the file\n",
    "            file_list.append(current_file)  \n",
    "            tmp_df = pd.read_csv(f)\n",
    "            if fill_nan == True:\n",
    "                tmp_df = tmp_df.fillna(0)\n",
    "            df_list.append(tmp_df)\n",
    "\n",
    "    return df_list, file_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de054ba0",
   "metadata": {},
   "source": [
    "Extract the training and validation data for each file into separate lists for y_regression and y_classification, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fbb707a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in the training dataset: ['P01', 'P02', 'P03', 'P04', 'P05', 'P07', 'P08', 'P09', 'P11', 'P12', 'P13', 'P14']\n",
      "Files in the validation dataset: ['P06', 'P10']\n"
     ]
    }
   ],
   "source": [
    "# Specify paths\n",
    "trainy_regression_path = '../Data/Prep/train/trainy-regression'\n",
    "trainy_classification_path = '../Data/Prep/train/trainy-classification'\n",
    "\n",
    "valy_regression_path = '../Data/Prep/validation/valy-regression'\n",
    "valy_classification_path = '../Data/Prep/validation/valy-classification'\n",
    "\n",
    "# Extract the data as dataframes stored into lists\n",
    "trainy_regression_list, train_file_list = extractdf(trainy_regression_path)  \n",
    "trainy_classification_list, _ = extractdf(trainy_classification_path, fill_nan=True)\n",
    "\n",
    "valy_regression_list, val_file_list = extractdf(valy_regression_path)\n",
    "valy_classification_list, _ = extractdf(valy_classification_path, fill_nan=True)\n",
    "\n",
    "print(\"Files in the training dataset:\", train_file_list)\n",
    "print(\"Files in the validation dataset:\", val_file_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e632e048",
   "metadata": {},
   "source": [
    "Intialize separate MinMaxScaler objects for the inputs and outputs. Scales the data between 0 and 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f77a6e0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MinMaxScaler()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MinMaxScaler()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_scaler = MinMaxScaler()\n",
    "\n",
    "# fit data to the first file in the training dataset\n",
    "y_scaler.fit(trainy_regression_list[0].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a2d27b",
   "metadata": {},
   "source": [
    "\"batchdata\" is a function to create random batches of data of a specified size for each classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d73224c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchdata(y_reg, y_class, batch_size):\n",
    "       \n",
    "    # Initialize a new array for the batch of inputs.\n",
    "    y_reg_batch, y_class_batch = [], []\n",
    "            \n",
    "    for k in range(batch_size):\n",
    "        # Get a random index value.\n",
    "        idx = np.random.randint(y_reg.shape[0])\n",
    "            \n",
    "        y_reg_batch.append(y_reg[idx])\n",
    "        y_class_batch.append(y_class[idx])\n",
    "        \n",
    "    return np.array(y_reg_batch), np.array(y_class_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001309fe",
   "metadata": {},
   "source": [
    "\"createsequences\" is a function to create mini-sequences (has no nans), reshaping data for training and evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c622cd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createsequences(y_reg_data, y_class_data, class_label):\n",
    "    \n",
    "    # initialize empty lists\n",
    "    y_reg, y_class = [], []\n",
    "    \n",
    "    # extract non-nan sequences that include the classification label\n",
    "    # (e.g., instrument panel, out the window, kneeboard) of interest\n",
    "    for i in range(y_reg_data.shape[0]):\n",
    "        if not np.isnan(y_reg_data[i]).any() and np.any(y_class_data[i] == class_label):\n",
    "            y_reg.append(y_reg_data[i])\n",
    "            y_class.append(y_class_data[i])\n",
    "     \n",
    "    if y_reg:\n",
    "        return np.array(y_reg), np.array(y_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981031b1",
   "metadata": {},
   "source": [
    "\"finaldataprep\" is a function which integrates the two functions above to prepare the datasets for training the neural network. The function returns 3D numpy arrays for the input, regression outputs, and classification outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e283ba51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finaldataprep(file_list, y_reg_list, y_class_list):\n",
    "    \n",
    "    # initialize empty lists\n",
    "    y_reg_out, y_class_out = [], []\n",
    "    # loop through the different object classifications and use the createsequences and \n",
    "    # batchdata functions to generate training data\n",
    "    tmp_y_reg_ph, tmp_y_class_ph = [],[]\n",
    "    sequence_size = []\n",
    "\n",
    "    for _, class_label in enumerate(np.unique(y_class_list[0].values)):\n",
    "        print(\"Current label is:\", class_label)\n",
    "        tmp_y_reg, tmp_y_class = [],[]\n",
    "        for i in range(len(file_list)):\n",
    "            y_reg_scaled = y_scaler.transform(y_reg_list[i].values) # MinMax Scale the y-regression data\n",
    "            y_class = y_class_list[i].values\n",
    "            # try to obtain sequences of the data for each file/object classification\n",
    "            try:\n",
    "                y_reg_seq, y_class_seq = createsequences(y_reg_scaled, y_class, class_label)\n",
    "                print(file_list[i], y_reg_seq.shape)\n",
    "\n",
    "                tmp_y_reg.append(y_reg_seq)\n",
    "                tmp_y_class.append(y_class_seq)\n",
    "            except Exception:\n",
    "                print(\"Could not retrieve data for:\", file_list[i])\n",
    "        # concatenate x, y-regression, and y-classififcation lists into numpy arrays\n",
    "        tmp_y_reg_ph.append(np.concatenate(tmp_y_reg, axis=0))\n",
    "        tmp_y_class_ph.append(np.concatenate(tmp_y_class, axis=0))\n",
    "\n",
    "        size_check = np.concatenate(tmp_y_reg, axis=0)\n",
    "        sequence_size.append(size_check.shape[0])\n",
    "\n",
    "    total_batch_size = max(sequence_size)\n",
    "    for i, class_label in enumerate(np.unique(y_class_list[0].values)):\n",
    "        batch_y_reg, batch_y_class = batchdata(tmp_y_reg_ph[i], tmp_y_class_ph[i], total_batch_size)\n",
    "        print(i, \"Train X Overall Shape =\", batch_y_reg.shape,\"\\n\")\n",
    "        y_reg_out.append(batch_y_reg)\n",
    "        y_class_out.append(batch_y_class)\n",
    "            \n",
    "\n",
    "    return (np.concatenate(y_reg_out, axis=0),\n",
    "           np.concatenate(y_class_out, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de5f5b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering sequences for the training dataset.\n",
      "\n",
      "Current label is: 0.0\n",
      "P01 (98, 3)\n",
      "P02 (107, 3)\n",
      "P03 (50, 3)\n",
      "P04 (37, 3)\n",
      "P05 (300, 3)\n",
      "P07 (62, 3)\n",
      "P08 (70, 3)\n",
      "P09 (22, 3)\n",
      "P11 (89, 3)\n",
      "P12 (12, 3)\n",
      "P13 (25, 3)\n",
      "P14 (276, 3)\n",
      "Current label is: 1.0\n",
      "P01 (1194, 3)\n",
      "P02 (2972, 3)\n",
      "P03 (2169, 3)\n",
      "P04 (8952, 3)\n",
      "P05 (6620, 3)\n",
      "P07 (5691, 3)\n",
      "P08 (690, 3)\n",
      "P09 (4300, 3)\n",
      "P11 (3013, 3)\n",
      "P12 (2586, 3)\n",
      "P13 (512, 3)\n",
      "P14 (7171, 3)\n",
      "Current label is: 2.0\n",
      "P01 (13914, 3)\n",
      "P02 (24086, 3)\n",
      "P03 (23042, 3)\n",
      "P04 (16317, 3)\n",
      "P05 (29277, 3)\n",
      "P07 (33428, 3)\n",
      "P08 (25769, 3)\n",
      "P09 (6728, 3)\n",
      "P11 (16874, 3)\n",
      "P12 (12265, 3)\n",
      "P13 (16578, 3)\n",
      "P14 (16405, 3)\n",
      "Current label is: 3.0\n",
      "P01 (10582, 3)\n",
      "P02 (9128, 3)\n",
      "P03 (8926, 3)\n",
      "P04 (4474, 3)\n",
      "P05 (8381, 3)\n",
      "P07 (7763, 3)\n",
      "P08 (3111, 3)\n",
      "P09 (9258, 3)\n",
      "P11 (3592, 3)\n",
      "P12 (11793, 3)\n",
      "P13 (10382, 3)\n",
      "P14 (6425, 3)\n",
      "Current label is: 4.0\n",
      "P01 (2933, 3)\n",
      "P02 (7891, 3)\n",
      "P03 (3843, 3)\n",
      "P04 (3180, 3)\n",
      "P05 (7622, 3)\n",
      "P07 (7206, 3)\n",
      "P08 (5912, 3)\n",
      "P09 (9724, 3)\n",
      "P11 (2642, 3)\n",
      "P12 (10573, 3)\n",
      "P13 (7079, 3)\n",
      "P14 (7665, 3)\n",
      "Current label is: 5.0\n",
      "P01 (5215, 3)\n",
      "P02 (3162, 3)\n",
      "P03 (1782, 3)\n",
      "P04 (3121, 3)\n",
      "P05 (2649, 3)\n",
      "P07 (3194, 3)\n",
      "P08 (399, 3)\n",
      "P09 (3692, 3)\n",
      "P11 (641, 3)\n",
      "P12 (2926, 3)\n",
      "P13 (4646, 3)\n",
      "P14 (852, 3)\n",
      "Current label is: 6.0\n",
      "P01 (1789, 3)\n",
      "P02 (403, 3)\n",
      "P03 (422, 3)\n",
      "P04 (49, 3)\n",
      "P05 (251, 3)\n",
      "P07 (68, 3)\n",
      "P08 (31, 3)\n",
      "P09 (366, 3)\n",
      "P11 (362, 3)\n",
      "P12 (61, 3)\n",
      "P13 (351, 3)\n",
      "P14 (76, 3)\n",
      "Current label is: 7.0\n",
      "P01 (222, 3)\n",
      "P02 (11, 3)\n",
      "P03 (27, 3)\n",
      "P04 (9, 3)\n",
      "P05 (42, 3)\n",
      "P07 (7, 3)\n",
      "P08 (5, 3)\n",
      "P09 (14, 3)\n",
      "P11 (17, 3)\n",
      "P12 (10, 3)\n",
      "P13 (31, 3)\n",
      "P14 (1, 3)\n",
      "Current label is: 8.0\n",
      "P01 (1145, 3)\n",
      "P02 (4859, 3)\n",
      "P03 (651, 3)\n",
      "P04 (355, 3)\n",
      "P05 (2064, 3)\n",
      "P07 (4138, 3)\n",
      "P08 (5, 3)\n",
      "P09 (2606, 3)\n",
      "P11 (368, 3)\n",
      "P12 (5479, 3)\n",
      "P13 (2755, 3)\n",
      "P14 (2937, 3)\n",
      "Current label is: 9.0\n",
      "P01 (11, 3)\n",
      "P02 (26, 3)\n",
      "P03 (15, 3)\n",
      "P04 (89, 3)\n",
      "P05 (89, 3)\n",
      "P07 (30, 3)\n",
      "Could not retrieve data for: P08\n",
      "P09 (48, 3)\n",
      "P11 (11, 3)\n",
      "P12 (128, 3)\n",
      "P13 (5, 3)\n",
      "P14 (265, 3)\n",
      "Current label is: 10.0\n",
      "P01 (3, 3)\n",
      "P02 (17, 3)\n",
      "P03 (1, 3)\n",
      "P04 (3, 3)\n",
      "P05 (261, 3)\n",
      "P07 (9, 3)\n",
      "P08 (30, 3)\n",
      "Could not retrieve data for: P09\n",
      "P11 (2, 3)\n",
      "P12 (16, 3)\n",
      "P13 (9, 3)\n",
      "P14 (43, 3)\n",
      "Current label is: 11.0\n",
      "P01 (1161, 3)\n",
      "P02 (582, 3)\n",
      "P03 (754, 3)\n",
      "P04 (441, 3)\n",
      "P05 (421, 3)\n",
      "P07 (231, 3)\n",
      "P08 (277, 3)\n",
      "P09 (389, 3)\n",
      "P11 (1130, 3)\n",
      "P12 (97, 3)\n",
      "P13 (677, 3)\n",
      "P14 (96, 3)\n",
      "Current label is: 12.0\n",
      "P01 (6080, 3)\n",
      "P02 (7052, 3)\n",
      "P03 (2341, 3)\n",
      "P04 (2728, 3)\n",
      "P05 (4604, 3)\n",
      "P07 (4570, 3)\n",
      "P08 (2650, 3)\n",
      "P09 (6127, 3)\n",
      "P11 (4476, 3)\n",
      "P12 (4669, 3)\n",
      "P13 (4504, 3)\n",
      "P14 (4260, 3)\n",
      "Current label is: 13.0\n",
      "P01 (19566, 3)\n",
      "P02 (12399, 3)\n",
      "P03 (21138, 3)\n",
      "P04 (9108, 3)\n",
      "P05 (12796, 3)\n",
      "P07 (10136, 3)\n",
      "P08 (25295, 3)\n",
      "P09 (32532, 3)\n",
      "P11 (13595, 3)\n",
      "P12 (24339, 3)\n",
      "P13 (12883, 3)\n",
      "P14 (31126, 3)\n",
      "0 Train X Overall Shape = (234683, 3) \n",
      "\n",
      "1 Train X Overall Shape = (234683, 3) \n",
      "\n",
      "2 Train X Overall Shape = (234683, 3) \n",
      "\n",
      "3 Train X Overall Shape = (234683, 3) \n",
      "\n",
      "4 Train X Overall Shape = (234683, 3) \n",
      "\n",
      "5 Train X Overall Shape = (234683, 3) \n",
      "\n",
      "6 Train X Overall Shape = (234683, 3) \n",
      "\n",
      "7 Train X Overall Shape = (234683, 3) \n",
      "\n",
      "8 Train X Overall Shape = (234683, 3) \n",
      "\n",
      "9 Train X Overall Shape = (234683, 3) \n",
      "\n",
      "10 Train X Overall Shape = (234683, 3) \n",
      "\n",
      "11 Train X Overall Shape = (234683, 3) \n",
      "\n",
      "12 Train X Overall Shape = (234683, 3) \n",
      "\n",
      "13 Train X Overall Shape = (234683, 3) \n",
      "\n",
      "Gathering sequences for the validation dataset.\n",
      "\n",
      "Current label is: 0.0\n",
      "P06 (17, 3)\n",
      "P10 (50, 3)\n",
      "Current label is: 1.0\n",
      "P06 (2144, 3)\n",
      "P10 (1369, 3)\n",
      "Current label is: 2.0\n",
      "P06 (21800, 3)\n",
      "P10 (10047, 3)\n",
      "Current label is: 3.0\n",
      "P06 (12119, 3)\n",
      "P10 (6975, 3)\n",
      "Current label is: 4.0\n",
      "P06 (8370, 3)\n",
      "P10 (4721, 3)\n",
      "Current label is: 5.0\n",
      "P06 (1660, 3)\n",
      "P10 (1717, 3)\n",
      "Current label is: 6.0\n",
      "P06 (69, 3)\n",
      "P10 (70, 3)\n",
      "Current label is: 7.0\n",
      "P06 (31, 3)\n",
      "P10 (7, 3)\n",
      "Current label is: 8.0\n",
      "P06 (1595, 3)\n",
      "P10 (1984, 3)\n",
      "Current label is: 9.0\n",
      "P06 (85, 3)\n",
      "P10 (38, 3)\n",
      "Current label is: 10.0\n",
      "P06 (40, 3)\n",
      "P10 (67, 3)\n",
      "Current label is: 11.0\n",
      "P06 (368, 3)\n",
      "P10 (284, 3)\n",
      "Current label is: 12.0\n",
      "P06 (4506, 3)\n",
      "P10 (2730, 3)\n",
      "Current label is: 13.0\n",
      "P06 (21899, 3)\n",
      "P10 (29280, 3)\n",
      "0 Train X Overall Shape = (51179, 3) \n",
      "\n",
      "1 Train X Overall Shape = (51179, 3) \n",
      "\n",
      "2 Train X Overall Shape = (51179, 3) \n",
      "\n",
      "3 Train X Overall Shape = (51179, 3) \n",
      "\n",
      "4 Train X Overall Shape = (51179, 3) \n",
      "\n",
      "5 Train X Overall Shape = (51179, 3) \n",
      "\n",
      "6 Train X Overall Shape = (51179, 3) \n",
      "\n",
      "7 Train X Overall Shape = (51179, 3) \n",
      "\n",
      "8 Train X Overall Shape = (51179, 3) \n",
      "\n",
      "9 Train X Overall Shape = (51179, 3) \n",
      "\n",
      "10 Train X Overall Shape = (51179, 3) \n",
      "\n",
      "11 Train X Overall Shape = (51179, 3) \n",
      "\n",
      "12 Train X Overall Shape = (51179, 3) \n",
      "\n",
      "13 Train X Overall Shape = (51179, 3) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Gathering sequences for the training dataset.\\n\")\n",
    "y_reg_train, y_class_train = finaldataprep(train_file_list, trainy_regression_list, trainy_classification_list)\n",
    "\n",
    "# delete list to free space for memory\n",
    "del trainy_regression_list, trainy_classification_list \n",
    "\n",
    "print(\"Gathering sequences for the validation dataset.\\n\")\n",
    "# concatenate the validation data after selection of random sequences\n",
    "y_reg_val, y_class_val = finaldataprep(val_file_list, valy_regression_list, valy_classification_list)\n",
    "\n",
    "# delete list to free space for memory\n",
    "del valy_regression_list, valy_classification_list "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bd4ab9",
   "metadata": {},
   "source": [
    "Gather stats on the training and validation datasets.   \n",
    "Including:  \n",
    "-Counts for each classification label in the training dataset  \n",
    "-Number of sequences in the training and validation datasets  \n",
    "-Check for missing values in the training and validation datasets  \n",
    "-Check the number of unique classification labels  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7dde36e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instrument Panel: 234683 \n",
      "Out the Window: 234683 \n",
      "Kneeboard: 234683\n",
      "The training data has 3285562 sequences.\n",
      "\n",
      "The validation data has 716506 sequences.\n",
      "y_regression_train Missing values: False\n",
      "y_classification_train Missing values: False\n",
      "y_regression_validation Missing values: False\n",
      "y_classification_validation Missing values: False\n",
      "\n",
      "All classification labels in the training dataset: [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13.]\n",
      "\n",
      "All classification labels in the validation dataset: [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13.]\n"
     ]
    }
   ],
   "source": [
    "# count the number of datapoints in the training dataset for the instrument panel, out the window, and kneeboard, respectively.\n",
    "ip_count = np.sum(y_class_train==0)\n",
    "kb_count = np.sum(y_class_train==2)\n",
    "outwin_count = np.sum(y_class_train==1)\n",
    "\n",
    "print(\"Instrument Panel:\", ip_count, \"\\nOut the Window:\", outwin_count, \"\\nKneeboard:\", kb_count)\n",
    "print(\"The training data has\", y_class_train.shape[0],\"sequences.\")\n",
    "print(\"\\nThe validation data has\", y_class_val.shape[0],\"sequences.\")\n",
    "\n",
    "# Check for nans in the training dataset\n",
    "y_reg_train_nans = np.isnan(y_reg_train).any()\n",
    "print(\"y_regression_train Missing values:\", y_reg_train_nans)\n",
    "y_class_train_nans = np.isnan(y_class_train).any()\n",
    "print(\"y_classification_train Missing values:\", y_class_train_nans)\n",
    "\n",
    "# Check for nans in the validation dataset\n",
    "y_reg_val_nans = np.isnan(y_reg_val).any()\n",
    "print(\"y_regression_validation Missing values:\", y_reg_val_nans)\n",
    "y_class_val_nans = np.isnan(y_class_val).any()\n",
    "print(\"y_classification_validation Missing values:\", y_class_val_nans)\n",
    "\n",
    "# check the number of unique classification labels\n",
    "number_classes_train = np.unique(y_class_train)\n",
    "print(\"\\nAll classification labels in the training dataset:\", number_classes_train)\n",
    "\n",
    "number_classes_val = np.unique(y_class_train)\n",
    "print(\"\\nAll classification labels in the validation dataset:\", number_classes_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9251bd4",
   "metadata": {},
   "source": [
    "Construct the classifier, train the model, and assess the training performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2f4136e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset states generated by Keras\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed8a3130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 3)]               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                64        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 16)                272       \n",
      "                                                                 \n",
      " classification_output (Dens  (None, 14)               238       \n",
      " e)                                                              \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 574\n",
      "Trainable params: 574\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Classification Model\n",
    "\n",
    "# input layer\n",
    "inputs = Input(shape=y_reg_train.shape[1])\n",
    "\n",
    "hidden = Dense(16, activation=\"relu\")(inputs)\n",
    "hidden = Dense(16, activation=\"relu\")(hidden)\n",
    "\n",
    "# output layer\n",
    "outputs = Dense(len(number_classes_train), activation='softmax', name=\"classification_output\")(hidden)\n",
    "\n",
    "# define the Model\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36d699ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training parameters for the classifier\n",
    "batch_size = 256\n",
    "optimizer = Adam(lr=1e-3)\n",
    "loss = 'sparse_categorical_crossentropy'\n",
    "metrics = 'acc'\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                        patience=10, verbose=1, restore_best_weights=True)\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35901531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "12835/12835 [==============================] - 17s 1ms/step - loss: 0.3068 - acc: 0.9323 - val_loss: 0.0755 - val_acc: 0.9837\n",
      "Epoch 2/200\n",
      "12835/12835 [==============================] - 18s 1ms/step - loss: 0.0300 - acc: 0.9941 - val_loss: 0.0232 - val_acc: 0.9986\n",
      "Epoch 3/200\n",
      "12835/12835 [==============================] - 18s 1ms/step - loss: 0.0122 - acc: 0.9980 - val_loss: 0.0131 - val_acc: 0.9993\n",
      "Epoch 4/200\n",
      "12835/12835 [==============================] - 18s 1ms/step - loss: 0.0075 - acc: 0.9988 - val_loss: 0.0081 - val_acc: 0.9998\n",
      "Epoch 5/200\n",
      "12835/12835 [==============================] - 19s 1ms/step - loss: 0.0055 - acc: 0.9990 - val_loss: 0.0064 - val_acc: 0.9998\n",
      "Epoch 6/200\n",
      "12835/12835 [==============================] - 19s 1ms/step - loss: 0.0045 - acc: 0.9992 - val_loss: 0.0053 - val_acc: 0.9995\n",
      "Epoch 7/200\n",
      "12835/12835 [==============================] - 19s 2ms/step - loss: 0.0039 - acc: 0.9992 - val_loss: 0.0047 - val_acc: 0.9999\n",
      "Epoch 8/200\n",
      "12835/12835 [==============================] - 19s 2ms/step - loss: 0.0034 - acc: 0.9993 - val_loss: 0.0033 - val_acc: 1.0000\n",
      "Epoch 9/200\n",
      "12835/12835 [==============================] - 19s 2ms/step - loss: 0.0031 - acc: 0.9994 - val_loss: 0.0029 - val_acc: 1.0000\n",
      "Epoch 10/200\n",
      "12835/12835 [==============================] - 20s 2ms/step - loss: 0.0028 - acc: 0.9994 - val_loss: 0.0031 - val_acc: 1.0000\n",
      "Epoch 11/200\n",
      "12835/12835 [==============================] - 20s 2ms/step - loss: 0.0026 - acc: 0.9994 - val_loss: 0.0021 - val_acc: 1.0000\n",
      "Epoch 12/200\n",
      "12835/12835 [==============================] - 20s 2ms/step - loss: 0.0025 - acc: 0.9994 - val_loss: 0.0033 - val_acc: 0.9999\n",
      "Epoch 13/200\n",
      "12835/12835 [==============================] - 20s 2ms/step - loss: 0.0023 - acc: 0.9994 - val_loss: 0.0036 - val_acc: 0.9988\n",
      "Epoch 14/200\n",
      "12835/12835 [==============================] - 20s 2ms/step - loss: 0.0023 - acc: 0.9994 - val_loss: 0.0020 - val_acc: 1.0000\n",
      "Epoch 15/200\n",
      "12835/12835 [==============================] - 20s 2ms/step - loss: 0.0022 - acc: 0.9994 - val_loss: 0.0017 - val_acc: 1.0000\n",
      "Epoch 16/200\n",
      "12835/12835 [==============================] - 20s 2ms/step - loss: 0.0021 - acc: 0.9994 - val_loss: 0.0090 - val_acc: 0.9951\n",
      "Epoch 17/200\n",
      "12835/12835 [==============================] - 20s 2ms/step - loss: 0.0020 - acc: 0.9995 - val_loss: 0.0012 - val_acc: 1.0000\n",
      "Epoch 18/200\n",
      "12835/12835 [==============================] - 20s 2ms/step - loss: 0.0019 - acc: 0.9995 - val_loss: 0.0012 - val_acc: 1.0000\n",
      "Epoch 19/200\n",
      "12835/12835 [==============================] - 20s 2ms/step - loss: 0.0019 - acc: 0.9995 - val_loss: 0.0016 - val_acc: 1.0000\n",
      "Epoch 20/200\n",
      "12835/12835 [==============================] - 20s 2ms/step - loss: 0.0018 - acc: 0.9995 - val_loss: 0.0042 - val_acc: 0.9981\n",
      "Epoch 21/200\n",
      "12835/12835 [==============================] - 20s 2ms/step - loss: 0.0018 - acc: 0.9995 - val_loss: 0.0010 - val_acc: 1.0000\n",
      "Epoch 22/200\n",
      "12835/12835 [==============================] - 20s 2ms/step - loss: 0.0018 - acc: 0.9995 - val_loss: 0.0012 - val_acc: 0.9998\n",
      "Epoch 23/200\n",
      "12835/12835 [==============================] - 20s 2ms/step - loss: 0.0017 - acc: 0.9995 - val_loss: 0.0010 - val_acc: 1.0000\n",
      "Epoch 24/200\n",
      "12835/12835 [==============================] - 20s 2ms/step - loss: 0.0017 - acc: 0.9995 - val_loss: 0.0013 - val_acc: 1.0000\n",
      "Epoch 25/200\n",
      "12835/12835 [==============================] - 20s 2ms/step - loss: 0.0017 - acc: 0.9995 - val_loss: 8.3101e-04 - val_acc: 1.0000\n",
      "Epoch 26/200\n",
      "12835/12835 [==============================] - 20s 2ms/step - loss: 0.0016 - acc: 0.9995 - val_loss: 0.0010 - val_acc: 1.0000\n",
      "Epoch 27/200\n",
      "12835/12835 [==============================] - 20s 2ms/step - loss: 0.0016 - acc: 0.9995 - val_loss: 6.4110e-04 - val_acc: 1.0000\n",
      "Epoch 28/200\n",
      "12835/12835 [==============================] - 20s 2ms/step - loss: 0.0016 - acc: 0.9996 - val_loss: 0.0014 - val_acc: 1.0000\n",
      "Epoch 29/200\n",
      "12835/12835 [==============================] - 20s 2ms/step - loss: 0.0016 - acc: 0.9995 - val_loss: 9.7485e-04 - val_acc: 1.0000\n",
      "Epoch 30/200\n",
      "12835/12835 [==============================] - 20s 2ms/step - loss: 0.0015 - acc: 0.9996 - val_loss: 5.3568e-04 - val_acc: 1.0000\n",
      "Epoch 31/200\n",
      "12835/12835 [==============================] - 20s 2ms/step - loss: 0.0015 - acc: 0.9995 - val_loss: 7.8566e-04 - val_acc: 1.0000\n",
      "Epoch 32/200\n",
      "12835/12835 [==============================] - 21s 2ms/step - loss: 0.0015 - acc: 0.9995 - val_loss: 6.7421e-04 - val_acc: 1.0000\n",
      "Epoch 33/200\n",
      "12835/12835 [==============================] - 20s 2ms/step - loss: 0.0014 - acc: 0.9996 - val_loss: 5.0876e-04 - val_acc: 1.0000\n",
      "Epoch 34/200\n",
      "12835/12835 [==============================] - 20s 2ms/step - loss: 0.0015 - acc: 0.9996 - val_loss: 0.0033 - val_acc: 0.9989\n",
      "Epoch 35/200\n",
      "12835/12835 [==============================] - 21s 2ms/step - loss: 0.0014 - acc: 0.9996 - val_loss: 4.1993e-04 - val_acc: 1.0000\n",
      "Epoch 36/200\n",
      "12835/12835 [==============================] - 21s 2ms/step - loss: 0.0014 - acc: 0.9996 - val_loss: 7.5154e-04 - val_acc: 1.0000\n",
      "Epoch 37/200\n",
      "12835/12835 [==============================] - 20s 2ms/step - loss: 0.0015 - acc: 0.9996 - val_loss: 5.1928e-04 - val_acc: 1.0000\n",
      "Epoch 38/200\n",
      "12835/12835 [==============================] - 20s 2ms/step - loss: 0.0014 - acc: 0.9996 - val_loss: 0.0011 - val_acc: 1.0000\n",
      "Epoch 39/200\n",
      "12835/12835 [==============================] - 21s 2ms/step - loss: 0.0013 - acc: 0.9996 - val_loss: 5.9752e-04 - val_acc: 1.0000\n",
      "Epoch 40/200\n",
      "12835/12835 [==============================] - 21s 2ms/step - loss: 0.0013 - acc: 0.9996 - val_loss: 3.9944e-04 - val_acc: 1.0000\n",
      "Epoch 41/200\n",
      "12835/12835 [==============================] - 20s 2ms/step - loss: 0.0014 - acc: 0.9996 - val_loss: 7.9178e-04 - val_acc: 1.0000\n",
      "Epoch 42/200\n",
      "12835/12835 [==============================] - 20s 2ms/step - loss: 0.0013 - acc: 0.9996 - val_loss: 0.0013 - val_acc: 1.0000\n",
      "Epoch 43/200\n",
      "12835/12835 [==============================] - 21s 2ms/step - loss: 0.0013 - acc: 0.9996 - val_loss: 0.0011 - val_acc: 1.0000\n",
      "Epoch 44/200\n",
      "12835/12835 [==============================] - 21s 2ms/step - loss: 0.0013 - acc: 0.9996 - val_loss: 3.9779e-04 - val_acc: 1.0000\n",
      "Epoch 45/200\n",
      "12835/12835 [==============================] - 21s 2ms/step - loss: 0.0013 - acc: 0.9996 - val_loss: 4.3430e-04 - val_acc: 1.0000\n",
      "Epoch 46/200\n",
      "12835/12835 [==============================] - 20s 2ms/step - loss: 0.0013 - acc: 0.9996 - val_loss: 4.6854e-04 - val_acc: 1.0000\n",
      "Epoch 47/200\n",
      "12835/12835 [==============================] - 21s 2ms/step - loss: 0.0013 - acc: 0.9996 - val_loss: 5.3422e-04 - val_acc: 1.0000\n",
      "Epoch 48/200\n",
      "12835/12835 [==============================] - 20s 2ms/step - loss: 0.0014 - acc: 0.9996 - val_loss: 9.3119e-04 - val_acc: 1.0000\n",
      "Epoch 49/200\n",
      "12835/12835 [==============================] - 21s 2ms/step - loss: 0.0013 - acc: 0.9996 - val_loss: 3.7626e-04 - val_acc: 1.0000\n",
      "Epoch 50/200\n",
      "12835/12835 [==============================] - 21s 2ms/step - loss: 0.0013 - acc: 0.9996 - val_loss: 6.5450e-04 - val_acc: 1.0000\n",
      "Epoch 51/200\n",
      "12835/12835 [==============================] - 21s 2ms/step - loss: 0.0013 - acc: 0.9996 - val_loss: 3.3483e-04 - val_acc: 1.0000\n",
      "Epoch 52/200\n",
      "12835/12835 [==============================] - 21s 2ms/step - loss: 0.0013 - acc: 0.9996 - val_loss: 0.0024 - val_acc: 0.9981\n",
      "Epoch 53/200\n",
      "12835/12835 [==============================] - 21s 2ms/step - loss: 0.0012 - acc: 0.9996 - val_loss: 3.0606e-04 - val_acc: 1.0000\n",
      "Epoch 54/200\n",
      "12835/12835 [==============================] - 21s 2ms/step - loss: 0.0012 - acc: 0.9996 - val_loss: 0.0083 - val_acc: 0.9960\n",
      "Epoch 55/200\n",
      "12835/12835 [==============================] - 21s 2ms/step - loss: 0.0012 - acc: 0.9996 - val_loss: 2.1445e-04 - val_acc: 1.0000\n",
      "Epoch 56/200\n",
      "12835/12835 [==============================] - 21s 2ms/step - loss: 0.0012 - acc: 0.9996 - val_loss: 0.0027 - val_acc: 0.9982\n",
      "Epoch 57/200\n",
      "12835/12835 [==============================] - 21s 2ms/step - loss: 0.0011 - acc: 0.9996 - val_loss: 2.5760e-04 - val_acc: 1.0000\n",
      "Epoch 58/200\n",
      "12835/12835 [==============================] - 21s 2ms/step - loss: 0.0012 - acc: 0.9996 - val_loss: 1.9256e-04 - val_acc: 1.0000\n",
      "Epoch 59/200\n",
      "12835/12835 [==============================] - 20s 2ms/step - loss: 0.0011 - acc: 0.9996 - val_loss: 2.3499e-04 - val_acc: 1.0000\n",
      "Epoch 60/200\n",
      "12835/12835 [==============================] - 20s 2ms/step - loss: 0.0012 - acc: 0.9996 - val_loss: 4.7073e-04 - val_acc: 1.0000\n",
      "Epoch 61/200\n",
      "12835/12835 [==============================] - 21s 2ms/step - loss: 0.0011 - acc: 0.9997 - val_loss: 2.6231e-04 - val_acc: 1.0000\n",
      "Epoch 62/200\n",
      "12835/12835 [==============================] - 20s 2ms/step - loss: 0.0012 - acc: 0.9996 - val_loss: 2.7440e-04 - val_acc: 1.0000\n",
      "Epoch 63/200\n",
      "12835/12835 [==============================] - 20s 2ms/step - loss: 0.0012 - acc: 0.9996 - val_loss: 4.2846e-04 - val_acc: 1.0000\n",
      "Epoch 64/200\n",
      "12835/12835 [==============================] - 20s 2ms/step - loss: 0.0011 - acc: 0.9997 - val_loss: 2.1728e-04 - val_acc: 1.0000\n",
      "Epoch 65/200\n",
      "12835/12835 [==============================] - 20s 2ms/step - loss: 0.0012 - acc: 0.9996 - val_loss: 6.1581e-04 - val_acc: 1.0000\n",
      "Epoch 66/200\n",
      "12835/12835 [==============================] - 20s 2ms/step - loss: 0.0012 - acc: 0.9996 - val_loss: 2.1240e-04 - val_acc: 1.0000\n",
      "Epoch 67/200\n",
      "12835/12835 [==============================] - 20s 2ms/step - loss: 0.0012 - acc: 0.9996 - val_loss: 8.6799e-04 - val_acc: 0.9998\n",
      "Epoch 68/200\n",
      "12802/12835 [============================>.] - ETA: 0s - loss: 0.0011 - acc: 0.9996Restoring model weights from the end of the best epoch.\n",
      "12835/12835 [==============================] - 21s 2ms/step - loss: 0.0011 - acc: 0.9996 - val_loss: 1.9491e-04 - val_acc: 1.0000\n",
      "Epoch 00068: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Train the classification model\n",
    "history = model.fit(y_reg_train, y_class_train,\n",
    "                    validation_data=(y_reg_val, y_class_val),\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True,\n",
    "                    epochs=200,\n",
    "                    verbose=1,\n",
    "                    callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5cfb57f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get training accuracy \n",
    "val_acc = history.history['val_acc']\n",
    "acc = history.history['acc']\n",
    "\n",
    "# Obtain number of epochs\n",
    "epochs = range(len(acc))\n",
    "\n",
    "# Plot loss and val loss\n",
    "plt.plot(val_acc[0:], label='validation_accuracy')\n",
    "plt.plot(acc[0:], label='training_accuracy')\n",
    "plt.legend()\n",
    "plt.title('Training Performance')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ffde1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\NAMRU User\\miniconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From C:\\Users\\NAMRU User\\miniconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: ../Models/Classifier\\assets\n"
     ]
    }
   ],
   "source": [
    "# Save model\n",
    "model.save(\"../Models/Classifier\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7d080166881b4e54330c01c7c798464d68da1aa03721536117721113f4213136"
  },
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "00d3cb7e298ecc8950ac69171eabeda5a9d90f66e167cf3ccb0bd870eaf61020"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
