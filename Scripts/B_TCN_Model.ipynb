{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "307c2795",
   "metadata": {},
   "source": [
    "#### Prediction of missing eye tracking data with neural networks\n",
    "Lucas D. Haberkamp<sup>1,2,3</sup>, Michael D. Reddix<sup>1</sup>\n",
    "\n",
    "<sup>1</sup>Naval Medical Research Unit - Dayton  \n",
    "<sup>2</sup>Oak Ridge Institute for Science and Education  \n",
    "<sup>3</sup>Leidos   \n",
    "\n",
    "---\n",
    "\n",
    "This script trains a temporal convolutional network to predict the 3D point of gaze coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04776dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import utils\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddd5d15",
   "metadata": {},
   "source": [
    "\"extractdf\" is a function which reads each file as a dataframe and saves the dataframe to a list and also saves a separate list of the filenames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14013f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill_nan should be used with classification ground truth data. prevents a nan label.\n",
    "def extractdf(data_path, fill_nan=False):\n",
    "    file_list, df_list = [], [] # initialize lists\n",
    "    for filename in os.listdir(data_path):\n",
    "        f = os.path.join(data_path, filename) \n",
    "        if os.path.isfile(f):\n",
    "            current_file = os.path.splitext(filename)[0].split('_')[0] # get the identifier of the participant from the file\n",
    "            file_list.append(current_file)  \n",
    "            tmp_df = pd.read_csv(f)\n",
    "            if fill_nan == True:\n",
    "                tmp_df = tmp_df.fillna(0)\n",
    "            df_list.append(tmp_df)\n",
    "\n",
    "    return df_list, file_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de054ba0",
   "metadata": {},
   "source": [
    "Extract the training and validation data for each file into separate lists for x and y_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbb707a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify paths\n",
    "trainx_path = '../Data/Prep/train/trainx'\n",
    "trainy_path = '../Data/Prep/train/trainy-regression'\n",
    "\n",
    "valx_path = '../Data/Prep/validation/valx'\n",
    "valy_path = '../Data/Prep/validation/valy-regression'\n",
    "\n",
    "# Extract the data as dataframes stored into lists\n",
    "trainx_list, train_file_list = extractdf(trainx_path)\n",
    "trainy_list, _ = extractdf(trainy_path)  \n",
    "\n",
    "valx_list, val_file_list = extractdf(valx_path)\n",
    "valy_list, _ = extractdf(valy_path)\n",
    "\n",
    "print(\"Files in the training dataset:\", train_file_list)\n",
    "print(\"Files in the validation dataset:\", val_file_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e632e048",
   "metadata": {},
   "source": [
    "Intialize separate MinMaxScaler objects for the inputs and outputs. Scales the data between 0 and 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77a6e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_scaler = MinMaxScaler()\n",
    "y_scaler = MinMaxScaler()\n",
    "\n",
    "# fit data to the first file in the training dataset\n",
    "x_scaler.fit(trainx_list[0].values)\n",
    "y_scaler.fit(trainy_list[0].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cecfd2f",
   "metadata": {},
   "source": [
    "Function to add gaussian noise to a signal. The scale is set as a proportion of the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce395775",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getnoise(x, scale):\n",
    "    \n",
    "    noise = []\n",
    "    mean = 0\n",
    "    mu = scale\n",
    "    for i in range(x.shape[1]):\n",
    "        random_vector = np.random.normal(mean, mu, x.shape[0])\n",
    "        noise.append(random_vector * np.std(x[:,i]))\n",
    "\n",
    "    noise = np.array(noise).transpose()\n",
    "    x = x + noise\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c441351",
   "metadata": {},
   "source": [
    "Function to prep the data by creating 3D input tensors, while not including nans in the 2D target tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab93e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataprep(file_list, x_list, y_list, addnoise, scale):\n",
    "    \n",
    "    batch_size = 1024\n",
    "    window_size = 256\n",
    "    shuffle_buffer_size = 1000\n",
    "    dataset_list = []\n",
    "    for i in range(len(file_list)):\n",
    "        if addnoise:\n",
    "            curr_x = getnoise(x_scaler.transform(x_list[i].values), scale)   \n",
    "        else:\n",
    "            curr_x = x_scaler.transform(x_list[i].values)   \n",
    "\n",
    "        curr_y = y_scaler.transform(y_list[i].values)\n",
    "\n",
    "        if i==0:\n",
    "            col_names = x_list[i].columns\n",
    "\n",
    "            tmp_x = x_scaler.transform(x_list[i].values)\n",
    "            for k in range(curr_x.shape[1]):\n",
    "                print(col_names[k])\n",
    "                plt.plot(curr_x[:1000,k])\n",
    "                plt.plot(tmp_x[:1000,k])\n",
    "                plt.show()\n",
    "                \n",
    "        x = tf.data.Dataset.from_tensor_slices(curr_x)\n",
    "        x = x.window(window_size, shift=1, drop_remainder=True)\n",
    "        x = x.flat_map(lambda w: w.batch(window_size))\n",
    "        \n",
    "        y = tf.data.Dataset.from_tensor_slices(curr_y[window_size-1:])\n",
    "        \n",
    "        filter_nan = lambda x, y: not tf.reduce_any(tf.math.is_nan(x)) and not tf.reduce_any(tf.math.is_nan(y))\n",
    "        \n",
    "        tmp_ds = tf.data.Dataset.zip((x,y)).filter(filter_nan)\n",
    "        dataset_list.append(tmp_ds)\n",
    "    \n",
    "    dataset = dataset_list[0]\n",
    "    \n",
    "    for j in range(1, len(dataset_list)):\n",
    "        dataset = dataset.concatenate(dataset_list[j])\n",
    "        \n",
    "    dataset = dataset.shuffle(shuffle_buffer_size)\n",
    "    \n",
    "    dataset = dataset.batch(batch_size).prefetch(1)\n",
    "    \n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9251bd4",
   "metadata": {},
   "source": [
    "Construct the the TCN, train the model, and assess the training performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f4136e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset states generated by Keras\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd9020b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the TCN model \n",
    "\n",
    "# list of dilation rates to be used in the model\n",
    "dilation_rates = [1,2,4,8,16,32,64,128]\n",
    "\n",
    "# input layer (Sequences have length 256 & 14 features)\n",
    "inputs = Input(shape=(256, 14))\n",
    "# intialize array to concatenate outputs from residual blocks with different dilations\n",
    "skips = []\n",
    "\n",
    "x = inputs\n",
    "# Generate an output for each residual block\n",
    "for dilation in dilation_rates:\n",
    "    # Inital 1x1 Convolution\n",
    "    x = Conv1D(32, kernel_size=1, padding='same', activation='relu')(x)\n",
    "\n",
    "    # Dilated Convolutions with tanh and sigmoid activations \n",
    "    x_f = Conv1D(8, kernel_size=2, padding=\"causal\", \n",
    "                 dilation_rate=dilation, activation='tanh')(x)\n",
    "    \n",
    "    x_g = Conv1D(8, kernel_size=2, padding=\"causal\", \n",
    "                 dilation_rate=dilation, activation='sigmoid')(x)\n",
    "    \n",
    "    # apply the gate\n",
    "    z = Multiply()([x_f, x_g])\n",
    "\n",
    "    # 1x1 convolution\n",
    "    z = Conv1D(32, kernel_size=1, padding='same', activation='relu')(z)\n",
    "    \n",
    "    # append output of the residual block\n",
    "    skips.append(z)\n",
    "\n",
    "    # make use of residual connection \n",
    "    x = Add()([x, z])\n",
    "\n",
    "# Add all the skip connections and pass through ReLU activation\n",
    "x = Activation('relu')(Add()(skips))\n",
    "\n",
    "# Predict output for each time step\n",
    "x = Conv1D(3, kernel_size=1, padding='same')(x)\n",
    "\n",
    "# function to return only the final gaze point label from the predictions\n",
    "def slice(x):\n",
    "    return x[:, -1, :]\n",
    "\n",
    "# Define output layer as the prediction for the final time step\n",
    "outputs = Lambda(slice)(x)\n",
    "\n",
    "# define model\n",
    "model = Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1a72af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training parameters for tcn\n",
    "optimizer = Adam(learning_rate=1e-3)\n",
    "loss = 'mae'\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                        patience=20, verbose=1, restore_best_weights=True)\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13209c31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train the model with an adaptive noise level (0.4 --> 0.2 --> 0.0)*stdev of the feature\n",
    "\n",
    "train_eval = [] # itialize list to store training history at each noise level\n",
    "\n",
    "for i in reversed(range(0,5,2)):\n",
    "    scale = i/10 # set the noise level\n",
    "\n",
    "    print(\"Gathering sequences for the training dataset.\\n\")\n",
    "    train_ds = dataprep(train_file_list, trainx_list, trainy_list, addnoise=True, scale=scale)\n",
    "    \n",
    "    print(\"Gathering sequences for the validation dataset.\\n\")\n",
    "    # concatenate the validation data after selection of random sequences\n",
    "    val_ds = dataprep(val_file_list, valx_list, valy_list, addnoise=False, scale=scale)\n",
    "    \n",
    "    print(\"Current noise level is:\", scale)\n",
    "\n",
    "    # Train the TCN\n",
    "    train_eval.append(model.fit(train_ds,\n",
    "                        validation_data=val_ds,\n",
    "                        epochs=200,\n",
    "                        verbose=1,\n",
    "                        callbacks=[early_stop]))\n",
    "\n",
    "    del train_ds, val_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eef893f",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_total = []\n",
    "loss_total = []\n",
    "\n",
    "for history in train_eval:\n",
    "    val_loss_total.append(history.history['val_loss'])\n",
    "    loss_total.append(history.history['loss'])\n",
    "\n",
    "val_loss_total = np.concatenate(val_loss_total, axis=0)\n",
    "loss_total = np.concatenate(loss_total, axis=0)\n",
    "\n",
    "# Obtain number of epochs\n",
    "epochs = range(len(loss_total))\n",
    "\n",
    "print(\"Total Epochs =\", len(epochs))\n",
    "\n",
    "# Plot loss and val loss\n",
    "plt.plot(val_loss_total, label='Validation Loss')\n",
    "plt.plot(loss_total, label='Training Loss')\n",
    "plt.legend()\n",
    "plt.title('Training Performance')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f4a788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model.save(\"../Models/TCN\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7d080166881b4e54330c01c7c798464d68da1aa03721536117721113f4213136"
  },
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "6eaee8bb16c720efd38a5149a321ecc0a6df516b2c421a256fc4f2fc4417c56d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
